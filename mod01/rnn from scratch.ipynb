{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from Scratch\n",
    "\n",
    "Original post at <a rel=\"canonical\" href=\"https://peterroelants.github.io/posts/rnn-implementation-part01/\">peterroelants.github.io</a> is generated from an IPython notebook file. [Link to the full IPython notebook file](https://github.com/peterroelants/peterroelants.github.io/blob/master/notebooks/RNN_implementation/rnn-implementation-part01.ipynb)\n",
    "\n",
    "This tutorial will illustrate how to implement a [simple RNN](#Linear-recurrent-neural-network) and how to train it with [Backpropagation through time](#Training-with-backpropagation-through-time) using [Rprop](#Rprop-optimisation) optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNNs) are neural nets that can deal with sequences of variable length (unlike feedforward nets). They are able to this by defining a [recurrence relation](https://en.wikipedia.org/wiki/Recurrence_relation) over timesteps which is typically the following formula:\n",
    "\n",
    "$$\n",
    "S_{k} = f(S_{k-1} \\cdot W_{rec} + X_k \\cdot W_x)\n",
    "$$\n",
    "\n",
    "Where $S_k$ is the state at time $k$, $X_k$ an input at time $k$, $W_{rec}$ and $W_x$ are parameters like the weights parameters in feedforward nets. Note that the RNN can be viewed as a state model with a [feedback loop](https://en.wikipedia.org/wiki/Feedback). \n",
    "\n",
    "The state evolves over time due to the recurrence relation, and the feedback is fed back into the state with a delay of one timestep. This delayed feedback loop gives the model memory because it can remember information between timesteps in the states.  \n",
    "The final output of the network $Y_k$ at a certain timestep $k$ is typically computed from one or more states $S_{k-i} \\cdots S_{k+j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN\n",
    "\n",
    "The first part of this tutorial describes a simple RNN that is trained to count how many 1's it sees on a binary input stream, and output the total count at the end of the sequence.\n",
    "\n",
    "The RNN model used here has one state, takes one input element from the binary stream each timestep, and outputs its last state at the end of the sequence. This model is shown in the figure below. The left part is a graphical illustration of the recurrence relation it describes ($ s_{k} = s_{k-1} \\cdot w_{rec} + x_k \\cdot w_x $). The right part illustrates how the network is unfolded through time over a sequence of length n. Notice that the unfolded network can be viewed as an (n+1)-layer neural network with the same shared parameters $w_{rec}$ and $w_x$ in each layer.\n",
    "\n",
    "![Structure of the linear RNN](https://peterroelants.github.io/images/RNN_implementation/SimpleRNN01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the seed for reproducability\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset\n",
    "\n",
    "The input data $X$ used in this example consists of 20 binary sequences of 10 timesteps each. Each input sequence is generated from a uniform random distribution which is rounded to $0$ or $1$.\n",
    "\n",
    "The output targets $t$ are the number of times '1' occurs in the sequence, which is equal to the sum of that sequence since the sequence is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "nb_of_samples = 20\n",
    "sequence_len = 10\n",
    "\n",
    "# Create the sequences\n",
    "X = np.zeros((nb_of_samples, sequence_len))\n",
    "for row_idx in range(nb_of_samples):\n",
    "    X[row_idx,:] = np.around(np.random.rand(sequence_len)).astype(int)\n",
    "\n",
    "# Create the targets for each sequence\n",
    "t = np.sum(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 1., 0., 0.],\n",
       "       [1., 1., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 1., 0., 1., 1., 0., 1.],\n",
       "       [1., 1., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 0., 1., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 1., 1., 1., 1.],\n",
       "       [1., 0., 0., 1., 0., 0., 1., 0., 1., 1.],\n",
       "       [1., 1., 1., 0., 0., 1., 0., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 0., 0., 1., 1., 0., 1.],\n",
       "       [0., 1., 1., 0., 1., 1., 0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 0., 1., 1., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0., 1., 0., 1., 0., 1., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 1., 0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [0., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 1., 0., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1., 1., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 4., 6., 6., 4., 4., 6., 5., 7., 5., 7., 6., 4., 7., 3., 6., 4.,\n",
       "       5., 6., 6.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with backpropagation through time\n",
    "\n",
    "The typical algorithm to train recurrent nets is the [backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time) algorithm. \n",
    "\n",
    "### Step 1: Compute the output with the forward step\n",
    "\n",
    "The forward step will unroll the network, and compute the forward activations just as in regular backpropagation. The final output will be used to compute the loss function from which the error signal used for training will be derived.\n",
    "\n",
    "When unfolding a recurrent net over multiple timesteps each layer computes the same recurrence relation on different timesteps. This recurrence relation for our model is defined in the `update_state` method.\n",
    "\n",
    "The `forward_states` method computes the states over increasing timesteps by applying the `update_state` method in a for-loop. \n",
    "\n",
    "Since the network begins without seeing anything of the sequence, an initial state needs to be provided. In this example, this initial state is set to $0$ (it is possible to treat it as another parameter).\n",
    "\n",
    "Finally, the loss $\\xi$ (`loss`) at the output is computed in this example via the mean squared error function (MSE) over all sequences in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward step functions\n",
    "\n",
    "def update_state(xk, sk, wx, wRec):\n",
    "    \"\"\"\n",
    "    Compute state k from the previous state (sk) and current \n",
    "    input (xk), by use of the input weights (wx) and recursive \n",
    "    weights (wRec).\n",
    "    \"\"\"\n",
    "    return xk * wx + sk * wRec\n",
    "\n",
    "\n",
    "def forward_states(X, wx, wRec):\n",
    "    \"\"\"\n",
    "    Unfold the network and compute all state activations \n",
    "    given the input X, input weights (wx), and recursive weights \n",
    "    (wRec). Return the state activations in a matrix, the last \n",
    "    column S[:,-1] contains the final activations.\n",
    "    \"\"\"\n",
    "    # Initialise the matrix that holds all states for all \n",
    "    #  input sequences. The initial state s0 is set to 0.\n",
    "\n",
    "    S = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "    \n",
    "    # Use the recurrence relation defined by update_state to update \n",
    "    #  the states trough time.\n",
    "    for k in range(0, X.shape[1]):\n",
    "        # S[k] = S[k-1] * wRec + X[k] * wx\n",
    "        S[:,k+1] = update_state(X[:,k], S[:,k], wx, wRec)\n",
    "    return S\n",
    "\n",
    "\n",
    "def loss(y, t): \n",
    "    \"\"\"MSE between the targets t and the outputs y.\"\"\"\n",
    "    return np.mean((t - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute the gradients with the backward step\n",
    "\n",
    "The backward step will begin with computing the gradient of the loss with respect to the output of the network $\\partial \\xi / \\partial y$ by the `output_gradient` method. This gradient will then be propagated backwards through time (layer by layer) from output to input to update the parameters by the `backward_gradient` method. The recurrence relation to propagate this gradient through the network can be written as:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial S_{k-1}} \n",
    "= \\frac{\\partial \\xi}{\\partial S_{k}} \\frac{\\partial S_{k}}{\\partial S_{k-1}}\n",
    "= \\frac{\\partial \\xi}{\\partial S_{k}} w_{rec}$$\n",
    "\n",
    "and starts at:\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial y} = \\frac{\\partial \\xi}{\\partial S_{n}}$$\n",
    "\n",
    "With $n$ the number of timesteps the netwoark is unfolded. Note that only the recursive parameter that connects states $w_{rec}$ plays a role in propagating the error down the network.\n",
    "\n",
    "The gradients of the loss function with respect to the parameters can then be found by summing the parameter gradients in each layer (or accumulating them while propagating the error).\n",
    "\n",
    "$$\\frac{\\partial \\xi}{\\partial w_x} \n",
    "= \\sum_{k=0}^{n} \\frac{\\partial \\xi}{\\partial S_{k}} x_k\n",
    "\\\\\n",
    "\\frac{\\partial \\xi}{\\partial w_{rec}} \n",
    "= \\sum_{k=1}^{n} \\frac{\\partial \\xi}{\\partial S_{k}} S_{k-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def output_gradient(y, t):\n",
    "    \"\"\"\n",
    "    Gradient of the MSE loss function with respect to the output y.\n",
    "    \"\"\"\n",
    "    return 2. * (y - t)\n",
    "\n",
    "\n",
    "def backward_gradient(X, S, grad_out, wRec):\n",
    "    \"\"\"\n",
    "    Backpropagate the gradient computed at the output (grad_out) \n",
    "    through the network. Accumulate the parameter gradients for \n",
    "    wX and wRec by for each layer by addition. Return the parameter \n",
    "    gradients as a tuple, and the gradients at the output of each layer.\n",
    "    \"\"\"\n",
    "    # Initialise the array that stores the gradients of the loss with \n",
    "    #  respect to the states.\n",
    "    grad_over_time = np.zeros((X.shape[0], X.shape[1]+1))\n",
    "    grad_over_time[:,-1] = grad_out\n",
    "    \n",
    "    # Set the gradient accumulations to 0\n",
    "    wx_grad = 0\n",
    "    wRec_grad = 0\n",
    "\n",
    "    for k in range(X.shape[1], 0, -1):\n",
    "        # Compute the parameter gradients and accumulate the results.\n",
    "        wx_grad += np.sum(\n",
    "            np.mean(grad_over_time[:,k] * X[:,k-1], axis=0))\n",
    "        wRec_grad += np.sum(\n",
    "            np.mean(grad_over_time[:,k] * S[:,k-1]), axis=0)\n",
    "        # Compute the gradient at the output of the previous layer\n",
    "        grad_over_time[:,k-1] = grad_over_time[:,k] * wRec\n",
    "    return (wx_grad, wRec_grad), grad_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: Rprop optimisation\n",
    "\n",
    "The gradient of a Vanilla RNN can be very unstable. Entering an area of unstable gradients could result in a huge jump on the loss surface, where the optimizer might end up far from the original point. This is illustrated in the following figure (from [Pascanu et al.](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf)):\n",
    "\n",
    "![Illustration exploding gradient from \"On the difficulty of training Recurrent Neural Networks\", Razvan Pascanu](https://peterroelants.github.io/images/RNN_implementation/ExplodingGradient_Razvan.png)\n",
    "\n",
    "Remember that the gradient descent update rule was:\n",
    "\n",
    "$$\n",
    "W(i+1) = W(i) - \\mu \\frac{\\partial \\xi}{\\partial W(i)}\n",
    "$$\n",
    "\n",
    "With $W(i)$ the value of $W$ at iteration $i$ during the gradient descent, and $\\mu$ the learning rate. \n",
    "\n",
    "If during training we would end up in the blue point in the surface plot above above ($w_x\\!=\\!1, w_{rec}\\!=\\!2$) the gradient would be in the order of $10^7$. Even with a small learning rate of $0.000001$ ($10^{-6}$) the parameters $W$ would be updated a distance 10 units from its current position, which would be catastrophic in our example. One way do deal with this is to lower the learning rate even more, but if then the optimisation enters a low gradient area the updates wouldn't move at all.\n",
    "\n",
    "Researchers have found many ways to do gradient based training in this unstable environment. Some examples are: [Gradient clipping](http://arxiv.org/pdf/1211.5063v2.pdf), [Hessian-Free Optimization](http://www.icml-2011.org/papers/532_icmlpaper.pdf), [Momentum](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf), etc.\n",
    "\n",
    "We can handle the unstable gradients by making the optimisation updates less sensitive to the gradients. One way to do this is by using a technique called [resilient backpropagation (Rprop)](https://en.wikipedia.org/wiki/Rprop). Rprop uses the sign of the gradient to determine the direction of update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Rprop optimisation function\n",
    "def update_rprop(X, t, W, W_prev_sign, W_delta, eta_p, eta_n):\n",
    "    \"\"\"\n",
    "    Update Rprop values in one iteration.\n",
    "    Args:\n",
    "        X: input data.\n",
    "        t: targets.\n",
    "        W: Current weight parameters.\n",
    "        W_prev_sign: Previous sign of the W gradient.\n",
    "        W_delta: Rprop update values (Delta).\n",
    "        eta_p, eta_n: Rprop hyperparameters.\n",
    "    Returns:\n",
    "        (W_delta, W_sign): Weight update and sign of last weight\n",
    "                           gradient.\n",
    "    \"\"\"\n",
    "    # Perform forward and backward pass to get the gradients\n",
    "    S = forward_states(X, W[0], W[1])\n",
    "    grad_out = output_gradient(S[:,-1], t)\n",
    "    W_grads, _ = backward_gradient(X, S, grad_out, W[1])\n",
    "    W_sign = np.sign(W_grads)  # Sign of new gradient\n",
    "\n",
    "    # Update the Delta (update value) for each weight \n",
    "    #  parameter seperately\n",
    "    for i, _ in enumerate(W):\n",
    "        if W_sign[i] == W_prev_sign[i]:\n",
    "            W_delta[i] *= eta_p\n",
    "        else:\n",
    "            W_delta[i] *= eta_n\n",
    "    return W_delta, W_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights are: wx = 1.0014,  wRec = 0.9997\n"
     ]
    }
   ],
   "source": [
    "# Perform Rprop optimisation\n",
    "\n",
    "# Set hyperparameters\n",
    "eta_p = 1.2\n",
    "eta_n = 0.5\n",
    "\n",
    "# Set initial parameters\n",
    "W = [-1.5, 2]  # [wx, wRec]\n",
    "W_delta = [0.001, 0.001]  # Update values (Delta) for W\n",
    "W_sign = [0, 0]  # Previous sign of W\n",
    "\n",
    "ls_of_ws = [(W[0], W[1])]  # List of weights to plot\n",
    "# Iterate over 500 iterations\n",
    "for i in range(500):\n",
    "    # Get the update values and sign of the last gradient\n",
    "    W_delta, W_sign = update_rprop(\n",
    "        X, t, W, W_sign, W_delta, eta_p, eta_n)\n",
    "    # Update each weight parameter seperately\n",
    "    for i, _ in enumerate(W):\n",
    "        W[i] -= W_sign[i] * W_delta[i]\n",
    "    ls_of_ws.append((W[0], W[1]))  # Add weights to list to plot\n",
    "\n",
    "print(f'Final weights are: wx = {W[0]:.4f},  wRec = {W[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model\n",
    "\n",
    "The final model is tested on a test sequence below. Note that the output is very close to the target output and that if we round the output to the nearest integer that the output would be perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target output: 5 vs Model output: 5.00\n"
     ]
    }
   ],
   "source": [
    "test_inpt = np.asmatrix([[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1]])\n",
    "test_outpt = forward_states(test_inpt, W[0], W[1])[:,-1]\n",
    "sum_test_inpt = test_inpt.sum()\n",
    "print((\n",
    "    f'Target output: {sum_test_inpt:d} vs Model output: '\n",
    "    f'{test_outpt[0]:.2f}'))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ready for more?\n",
    "\n",
    "LSTM implemented in numpy: https://github.com/nicodjimenez/lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
